{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import gc \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory(): \n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    print(t, r, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprint_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m, in \u001b[0;36mprint_memory\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_memory\u001b[39m(): \n\u001b[0;32m----> 2\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtotal_memory\n\u001b[1;32m      3\u001b[0m     r \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_reserved(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m     a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/lib/python3.8/site-packages/torch/cuda/__init__.py:359\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the properties of a device.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[1;32m    360\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/lib/python3.8/site-packages/torch/cuda/__init__.py:217\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    221\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "print_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train_evaluation = pd.read_csv('sales_train_evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aggregated_dataframe(sales_train_evaluation):\n",
    "    # Columns to aggregate are d_1 to d_1942\n",
    "    cols_to_agg = [f'd_{i}' for i in range(1, 1942)]\n",
    "\n",
    "    # defines how we are grouping \n",
    "    group_cols = [\n",
    "        ['state_id'],\n",
    "        ['store_id'],\n",
    "        ['cat_id'],\n",
    "        ['dept_id'],\n",
    "        ['state_id', 'cat_id'],\n",
    "        ['state_id', 'dept_id'],\n",
    "        ['store_id', 'cat_id'],\n",
    "        ['store_id', 'dept_id'],\n",
    "        ['item_id'],\n",
    "        ['item_id', 'state_id']\n",
    "    ]\n",
    "    \n",
    "    # Create a copy of the original DataFrame to start aggregating into\n",
    "    aggregated_df = sales_train_evaluation.copy()\n",
    "    aggregated_df['Hierarchy Level'] = len(group_cols)  # Set NaN for original data\n",
    "\n",
    "    # Store the indices of the original rows\n",
    "    original_indices = np.arange(len(aggregated_df))\n",
    "\n",
    "    # Perform aggregation and append to the original DataFrame\n",
    "    hierarchy_level = 0\n",
    "    appended_data = []\n",
    "    for group in group_cols:\n",
    "        group_df = sales_train_evaluation.groupby(group)[cols_to_agg].sum().reset_index()\n",
    "        group_df['Hierarchy Level'] = hierarchy_level\n",
    "        appended_data.append(group_df)\n",
    "        hierarchy_level += 1\n",
    "\n",
    "    # Append all aggregated data in the order of hierarchy (lower index first)\n",
    "    aggregated_df = pd.concat([aggregated_df] + appended_data, ignore_index=True)\n",
    "    \n",
    "    # Sort DataFrame by 'Hierarchy Level'\n",
    "    aggregated_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Create an empty aggregation matrix\n",
    "    n_rows = len(aggregated_df)\n",
    "    agg_matrix = np.zeros((n_rows, n_rows), dtype=int)\n",
    "\n",
    "    # Mark original rows (self aggregation)\n",
    "    np.fill_diagonal(agg_matrix[:len(original_indices), :len(original_indices)], 1)\n",
    "\n",
    "    # Fill in the aggregation matrix for the appended rows\n",
    "    new_index_offset = len(original_indices)\n",
    "    for group in group_cols:\n",
    "        group_indices = sales_train_evaluation.groupby(group).apply(lambda x: x.index.tolist()).tolist()\n",
    "        for sub_indices in group_indices:\n",
    "            agg_matrix[new_index_offset, sub_indices] = 1\n",
    "            new_index_offset += 1\n",
    "\n",
    "    return aggregated_df, agg_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df, agg_matrix = create_aggregated_dataframe(sales_train_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_agg = [f'd_{i}' for i in range(1, 1942)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = agg_df[cols_to_agg].T\n",
    "\n",
    "maximum = np.max(data.values)\n",
    "data_scaled = (data / maximum).values\n",
    "\n",
    "seed = 0 \n",
    "\n",
    "def set_seeds(seed): \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # if using nvidia gpu\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "\n",
    "set_seeds(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1941, 42839)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1941, 42839)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_series = data.shape[1] \n",
    "n_total = data.shape[0]\n",
    "n_train = 1400\n",
    "context_window = 2\n",
    "n_val = 150\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wmape(actual_values, forecasted_values):\n",
    "    # compute wMAPE metric\n",
    "    n = len(actual_values)\n",
    "    num = np.sum(np.abs(actual_values - forecasted_values))\n",
    "    den = np.sum(np.abs(actual_values))\n",
    "    wmape = num/den\n",
    "    return wmape\n",
    "\n",
    "def calculate_RMSE(actual_values, forecasted_values): \n",
    "    # compute RMSE metric\n",
    "    squared_errors = (actual_values - forecasted_values) ** 2\n",
    "    mean_squared_error = np.mean(squared_errors)\n",
    "    rmse = np.sqrt(mean_squared_error)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-250.,    0.,  250.,  500.,  750., 1000., 1250., 1500., 1750.,\n",
       "        2000., 2250.]),\n",
       " [Text(-250.0, 0, '−250'),\n",
       "  Text(0.0, 0, '0'),\n",
       "  Text(250.0, 0, '250'),\n",
       "  Text(500.0, 0, '500'),\n",
       "  Text(750.0, 0, '750'),\n",
       "  Text(1000.0, 0, '1000'),\n",
       "  Text(1250.0, 0, '1250'),\n",
       "  Text(1500.0, 0, '1500'),\n",
       "  Text(1750.0, 0, '1750'),\n",
       "  Text(2000.0, 0, '2000'),\n",
       "  Text(2250.0, 0, '2250')])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data_scaled[:,0])\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(dataset, range_, context_window): \n",
    "    # produces X_data and y_data tensors given the dataset \n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    for i in range_:\n",
    "        X = dataset[i:i+context_window,:].T.flatten()\n",
    "        X_data.append(X)\n",
    "\n",
    "        y = dataset[i+context_window:i+context_window+1,:].T.flatten()\n",
    "        y_data.append(y)\n",
    "\n",
    "    return torch.tensor(X_data, dtype=torch.float32), torch.tensor(y_data, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_M5(Dataset):\n",
    "    def __init__(self, root_path, flag='train', context_window=context_window, n_train=n_train, n_val=n_val, seq_len=n_train-context_window):\n",
    "        \n",
    "        # init\n",
    "        self.root_path = root_path\n",
    "        self.context_window=context_window\n",
    "        self.n_train = n_train\n",
    "        self.n_val = n_val\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        assert flag in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[flag]\n",
    "\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        df_raw = agg_df[cols_to_agg].T\n",
    "        maximum = np.max(df_raw.values)\n",
    "        data = (df_raw / maximum).values\n",
    "        \n",
    "        X_data = []\n",
    "        y_data = []\n",
    "\n",
    "        if self.set_type == 0: \n",
    "            data_range = range(self.n_train)\n",
    "        elif self.set_type == 1: \n",
    "            data_range = range(self.n_train, self.n_train + self.n_val)\n",
    "        else: \n",
    "            data_range = range(self.n_train + self.n_val, self.data.shape[0] - self.context_window)\n",
    "\n",
    "        for i in data_range:\n",
    "            X = data[i:i+self.context_window,:].T.flatten()\n",
    "            X_data.append(X)\n",
    "\n",
    "            y = data[i+self.context_window:i+self.context_window+1,:].T.flatten()\n",
    "            y_data.append(y)\n",
    "\n",
    "        self.X = torch.tensor(np.array(X_data)).float()\n",
    "        self.y = torch.tensor(np.array(y_data)).float()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index:index + self.seq_len, :]\n",
    "        y = self.y[index:index + self.seq_len]\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) - context_window - self.seq_len + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset_M5('/home/gridsan/mhensgen/m5', seq_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 85678])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/slurm_tmp/25983519.0.0/ipykernel_1262672/3537104489.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  return torch.tensor(X_data, dtype=torch.float32), torch.tensor(y_data, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1400, 85678])\n",
      "torch.Size([150, 85678])\n",
      "torch.Size([389, 85678])\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = make_data(data_scaled, range(n_train), context_window)\n",
    "X_val, y_val = make_data(data_scaled, range(n_train, n_train+n_val), context_window)\n",
    "X_test, y_test = make_data(data_scaled, range(n_train + n_val,n_total - context_window), context_window)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_level_0 = agg_df[agg_df['Hierarchy Level'] == 10].shape[0]\n",
    "\n",
    "aggregation_mat = torch.from_numpy(agg_matrix[num_level_0:, :]).float().to(device)\n",
    "S = agg_matrix[:, :num_level_0]\n",
    "\n",
    "def coherency_loss(network): \n",
    "    # computes coherency on last layer of the network \n",
    "    return torch.norm(aggregation_mat @ network.last_layer.weight - network.last_layer.weight[num_level_0:, :]) + torch.norm(\n",
    "        aggregation_mat @ network.last_layer.bias - network.last_layer.bias[num_level_0:]) \n",
    "\n",
    "def coherency_metric(predictions):\n",
    "    # computes the actual coherency of predictions \n",
    "    return torch.norm(predictions.T[num_level_0:, :] - aggregation_mat @ predictions.T) / predictions.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprint_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m, in \u001b[0;36mprint_memory\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_memory\u001b[39m(): \n\u001b[0;32m----> 2\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtotal_memory\n\u001b[1;32m      3\u001b[0m     r \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_reserved(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m     a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/lib/python3.8/site-packages/torch/cuda/__init__.py:359\u001b[0m, in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the properties of a device.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[1;32m    360\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/lib/python3.8/site-packages/torch/cuda/__init__.py:217\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    221\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "print_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_M(S): \n",
    "    # creates the projection matrix M given the aggregation matrix S\n",
    "    m, m_K = S.shape\n",
    "    m_agg = m-m_K\n",
    "\n",
    "    # The top `m_agg` rows of the matrix `S` give the aggregation constraint matrix.\n",
    "    S_agg = S[num_level_0:, :]\n",
    "    A = np.hstack((-S_agg, np.eye(m_agg)))\n",
    "\n",
    "    M = torch.eye(m) - torch.from_numpy(A.T @ np.linalg.pinv(A @ A.T) @ A).float().to(device)\n",
    "\n",
    "    return M, A\n",
    "\n",
    "M, A = create_M(S)\n",
    "\n",
    "def project_samples(y): \n",
    "    return torch.matmul(y, M)\n",
    "\n",
    "\n",
    "def least_squares_loss(predictions): \n",
    "    # computes the projections and then takes least squares loss between predictions and projections \n",
    "    # predictions should have shape n_samples x n_series \n",
    "    # implements coherency loss from https://www.sciencedirect.com/science/article/pii/S0306261923008747#fig5\n",
    "    projections = project_samples(predictions)\n",
    "    error = predictions - projections \n",
    "    square_error = torch.square(error)\n",
    "    return torch.sum(square_error) / error.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val.to(device) \n",
    "y_val = y_val.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmNet(nn.Module):\n",
    "    def __init__(self, n_series, context, should_project=False):\n",
    "        \"\"\"\n",
    "        n_series = number of time series we have\n",
    "        context = size of context window (how many samples back we are using to predict)\n",
    "        should_project: boolean determining if we use a projection before output for coherency\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        lstm_size = int(n_series * context / 1000)\n",
    "        lstm_hidden_size = int(lstm_size / 2)\n",
    "        last_layer_input_size = int(n_series / 1000)\n",
    "\n",
    "        self.encoder = nn.Linear(n_series * context, lstm_size, dtype=torch.float32)\n",
    "        self.lstm =  nn.LSTM(input_size=lstm_size, hidden_size=lstm_hidden_size, dropout=0.2, num_layers=2, dtype=torch.float32)\n",
    "        self.decoder = nn.Linear(lstm_hidden_size, last_layer_input_size, dtype=torch.float32)\n",
    "        self.last_layer = nn.Linear(last_layer_input_size, n_series, dtype=torch.float32) \n",
    "        \n",
    "        self.should_project = should_project\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batch_norm = nn.BatchNorm1d(lstm_hidden_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        encoded = self.relu(encoded) \n",
    "        lstm_out, _ = self.lstm(encoded)\n",
    "        lstm_out = self.relu(lstm_out)\n",
    "        #x = self.batch_norm(x)\n",
    "        decoded = self.relu(self.decoder(lstm_out))\n",
    "        out = self.last_layer(decoded)\n",
    "        if self.should_project: \n",
    "            out = project_samples(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "def train_net(n_epochs, batch_size=10, coherent=False, alpha=0, should_project=False, alpha_scaling=0, least_squares=False, scaling_increase_factor=0,\n",
    "               verbose=False, should_schedule=False, plot_loss=False, lr=0.001, max_grad_norm=10, update_alpha_epoch=100): \n",
    "    \"\"\"\n",
    "    trains an LSTMNet on the traffic dataset based on the given parameters\n",
    "    coherent: if true, uses coherent loss \n",
    "    alpha: the term we multiply the 'coherency_loss' by in calculating overall loss \n",
    "    should_project: if true, uses projection method \n",
    "    alpha_scaling: if a number, we will scale alpha by alpha_scaling * l1/l2. \n",
    "    essentially, we are setting l1 = l2*alpha, so the coherency will be weighted by 'alpha_scaling' in the loss\n",
    "    \"\"\"\n",
    "    # sets the number of epochs we train before updating alpha \n",
    "\n",
    "    network = LstmNet(n_series, context_window, should_project=should_project)\n",
    "    network = network.to(device)\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False)\n",
    "\n",
    "    print('--- alpha {alpha} ---- lr  {lr}----'.format(alpha=alpha, lr=lr))\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(network.parameters(), lr=lr)\n",
    "    if should_schedule: \n",
    "        if scaling_increase_factor > 0: \n",
    "            scheduler = MultiStepLR(optimizer, milestones=[i for i in range(update_alpha_epoch + 1, n_epochs, update_alpha_epoch)], gamma=1-scaling_increase_factor)\n",
    "        else: \n",
    "            scheduler = MultiStepLR(optimizer, milestones=[750, 1500, 2250], gamma=0.99)\n",
    "\n",
    "    losses = []\n",
    "    l1s = [] \n",
    "    l2s = []\n",
    "    validation_loss =[] \n",
    "    \n",
    "    for epoch in tqdm(range(n_epochs)):  # loop over the dataset multiple times\n",
    "\n",
    "        inputs, labels = next(iter(train_dataloader))\n",
    "        \n",
    "        inputs = inputs.to(device) \n",
    "        labels = labels.to(device) \n",
    "\n",
    "        # print(inputs[0, :20, 0])\n",
    "        # print(labels[0, :20, 0]) \n",
    "        # print(X_train[:20, 0]) \n",
    "        # print(y_train[:20, 0]) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = network(inputs)\n",
    "\n",
    "        # add coherency network loss\n",
    "        if coherent: \n",
    "            l1 = criterion(outputs, labels) \n",
    "            l2 = coherency_loss(network)\n",
    "\n",
    "            l2s.append(l2.item())\n",
    "            l1s.append(l1.item())\n",
    "\n",
    "            loss = (l1 + alpha * l2)\n",
    "\n",
    "            if loss.isnan(): \n",
    "                print('---')\n",
    "                print(l1)\n",
    "                print(l1s[epoch-5:])\n",
    "                print(l2)\n",
    "                print(l2s[epoch-5:])\n",
    "\n",
    "            validation_loss.append((criterion(network(X_val.to(device)), y_val.to(device)) + alpha * l2).item())\n",
    "        \n",
    "        elif least_squares: \n",
    "            l1 = criterion(outputs, labels) \n",
    "            l2 = 0\n",
    "            # TODO: speed this up with batching \n",
    "            for i in range(outputs.shape[0]): \n",
    "                l2 = l2 + torch.norm(outputs[i, :, :].T[num_level_0:, :] - aggregation_mat @ outputs[i, :, :].T)\n",
    "\n",
    "            l2s.append(l2.item())\n",
    "            l1s.append(l1.item())\n",
    "\n",
    "            loss = (l1 + alpha * l2)\n",
    "\n",
    "            if loss.isnan(): \n",
    "                print('---')\n",
    "                print(l1)\n",
    "                print(l1s[epoch-5:])\n",
    "                print(l2)\n",
    "                print(l2s[epoch-5:])\n",
    "\n",
    "            validation_loss.append((criterion(network(X_val.to(device)), y_val.to(device)) + alpha * l2).item())\n",
    "\n",
    "        # normal loss\n",
    "        else: \n",
    "            loss = criterion(outputs, labels)\n",
    "            validation_loss.append(criterion(network(X_val.to(device)), y_val.to(device)).item())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(network.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if epoch % update_alpha_epoch == 0 and epoch > 0:   \n",
    "            if verbose:  \n",
    "                print(\"Epoch:\", epoch, \" Loss: \", np.mean(losses))\n",
    "                print(np.mean(np.array(l1s[epoch-update_alpha_epoch:epoch])))\n",
    "                print(np.mean(np.array(l2s[epoch-update_alpha_epoch:epoch])))\n",
    "            # plt.figure()\n",
    "            # plt.plot(range(len(losses[epoch-100:epoch])), losses[epoch-100:epoch], label='training')\n",
    "            # plt.plot(range(len(validation_loss[epoch-100:epoch])), validation_loss[epoch-100:epoch], label='validation')\n",
    "            # plt.legend()\n",
    "            # plt.show()\n",
    "\n",
    "            if (coherent or least_squares) and (alpha_scaling or scaling_increase_factor > 0): \n",
    "                with torch.no_grad():\n",
    "                    l1_mean = np.mean(np.array(l1s[epoch-update_alpha_epoch:epoch]))\n",
    "                    l2_mean =  np.mean(np.array(l2s[epoch-update_alpha_epoch:epoch]))\n",
    "                    print(l1_mean)\n",
    "                    print(l2_mean)\n",
    "                    print(max(np.array(l2s[epoch-update_alpha_epoch:epoch])))\n",
    "                    alpha = l1_mean/l2_mean * alpha_scaling\n",
    "                    alpha_scaling = alpha_scaling + scaling_increase_factor\n",
    "                    if verbose: \n",
    "                        print('scaled: ', l1_mean/l2_mean)\n",
    "                        print('RMSE mean: ', l1_mean)\n",
    "                        print('Coherency Loss mean: ', l2_mean)\n",
    "                        print('alpha: ', alpha)\n",
    "                        print('alpha scaling: ', alpha_scaling)\n",
    "\n",
    "        if should_schedule: \n",
    "            scheduler.step()\n",
    "\n",
    "            \n",
    "    # plotting \n",
    "    plot_start = 0\n",
    "    start_2 = 101\n",
    "    if plot_loss: \n",
    "        plt.plot(range(len(losses[plot_start:])), losses[plot_start:], label='Train')\n",
    "        plt.plot(range(len(losses[plot_start:])), validation_loss[plot_start:], label='val')\n",
    "        plt.title('RMSE starting at epoch {plot_start}'.format(plot_start=plot_start))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        if coherent: \n",
    "            plt.plot(range(len(l2s[plot_start:])), l2s[plot_start:], label='Coherency Loss')\n",
    "            plt.title('Coherency Loss loss starting at epoch {plot_start}'.format(plot_start=plot_start))\n",
    "            plt.legend() \n",
    "            plt.show()\n",
    "        if start_2: \n",
    "            plt.plot(range(len(losses[start_2:])-4), [(losses[i] + losses[i + 1] + losses[i + 2] + losses[i + 3] + losses[i + 4])/5 for i in range(start_2, len(losses)-4)], label='Train')\n",
    "            plt.plot(range(len(losses[start_2:])-6), [(validation_loss[i] + validation_loss[i + 1] + validation_loss[i + 2] + validation_loss[i + 3] + validation_loss[i + 4] + validation_loss[i + 5] + validation_loss[i + 6])/7 for i in range(start_2, len(losses)-6)], label='val')\n",
    "            plt.title('RMSE starting at epoch {plot_start}'.format(plot_start=start_2))\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            if coherent: \n",
    "                plt.plot(range(len(l2s[start_2:])), l2s[start_2:], label='Coherency Loss')\n",
    "                plt.title('Coherency loss starting at epoch {plot_start}'.format(plot_start=start_2))\n",
    "                plt.legend() \n",
    "                plt.show()\n",
    "    \n",
    "    return network, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(ts, networks, labels, starting_zone = 0): \n",
    "    \"\"\"\n",
    "    Visualizes the networks' predictions on a specific time series \n",
    "    ts: time series to visualize\n",
    "    labels: labels of the networks for plotting\n",
    "    defining each zone as 50 points just so the graphs are easier to see \n",
    "    networks and labels should have same length\n",
    "    \"\"\"\n",
    "    with torch.no_grad(): \n",
    "        truth = y_test.numpy()[:,ts]\n",
    "        start_time = starting_zone*50\n",
    "        end_time = start_time + 50\n",
    "        plt.plot(truth[start_time:end_time], label = 'truth')\n",
    "\n",
    "        for i in range(len(networks)): \n",
    "            predict_net = networks[i](X_test).numpy()[:,ts]\n",
    "\n",
    "            plt.plot(predict_net[start_time:end_time], label = labels[i])\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_RMSE(network): \n",
    "    network.lstm.eval()  \n",
    "    return calculate_RMSE(y_test.cpu().numpy(), network(X_test.to(device)).cpu().detach().numpy())\n",
    "\n",
    "\n",
    "def get_metrics(networks, verbose=False): \n",
    "    rmses = []\n",
    "    network_coherencies = [] \n",
    "    ll_losses = [] \n",
    "    #print(networks)\n",
    "    for network in networks: \n",
    "        network.lstm.eval() \n",
    "        rmse = calculate_mean_RMSE(network)\n",
    "        network_coherency = coherency_metric(network(X_test.to(device))).item() \n",
    "        ll_loss = coherency_loss(network).item()\n",
    "        if verbose: \n",
    "            print(\"RMSE: \", rmse)\n",
    "            print(\"Network coherency: \", network_coherency)\n",
    "            print(\"Last Layer Coherency Loss: \", ll_loss)\n",
    "        rmses.append(rmse)\n",
    "        network_coherencies.append(network_coherency)\n",
    "        ll_losses.append(ll_loss)\n",
    "    return rmses, network_coherencies, ll_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, name): \n",
    "    torch.save(model.state_dict(), name + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path, should_project): \n",
    "    model = LstmNet(n_series, context_window, should_project=should_project)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.to(device)\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory(): \n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    print(t, r, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34079637504 9460252672 2193185792\n"
     ]
    }
   ],
   "source": [
    "print_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_baselines = True\n",
    "trained_projection = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.0026053598\n",
      "Network coherency:  0.0002604941837489605\n",
      "Last Layer Coherency Loss:  356.9902648925781\n",
      "RMSE:  0.0024430952\n",
      "Network coherency:  0.00011353904847055674\n",
      "Last Layer Coherency Loss:  373.8934020996094\n",
      "RMSE:  0.0025201256\n",
      "Network coherency:  0.00017331323761027306\n",
      "Last Layer Coherency Loss:  348.4934997558594\n",
      "RMSE:  0.0024936586\n",
      "Network coherency:  0.00022045493824407458\n",
      "Last Layer Coherency Loss:  370.22509765625\n",
      "RMSE:  0.0024930865\n",
      "Network coherency:  0.00016706177848391235\n",
      "Last Layer Coherency Loss:  365.2541809082031\n"
     ]
    }
   ],
   "source": [
    "n_trials = 5\n",
    "\n",
    "baseline_metrics = np.zeros((4, n_trials))\n",
    "baseline_networks = [] \n",
    "\n",
    "for i in range(n_trials): \n",
    "    seed = random.randint(0, 100) \n",
    "    if trained_baselines: \n",
    "        baseline_network = load_model('baseline_batching_{i}.pth'.format(i=i), False)\n",
    "    else:\n",
    "        baseline_network, loss = train_net(500, batch_size=25, plot_loss=True, lr=0.0005)\n",
    "        save_model(baseline_network, 'baseline_batching_{i}'.format(i=i))\n",
    "        \n",
    "    rmse_baseline, network_coherency_baseline, network_loss_baseline = get_metrics([baseline_network], verbose=True)\n",
    "    validation_loss = calculate_RMSE(y_val.cpu().numpy(), baseline_network(X_val.to(device)).detach().cpu().numpy())\n",
    "    baseline_metrics[:, i] = [rmse_baseline[0], network_coherency_baseline[0], network_loss_baseline[0], validation_loss]\n",
    "    baseline_networks.append(baseline_network) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.0024419057\n",
      "Network coherency:  1.8960058412176295e-08\n",
      "Last Layer Coherency Loss:  384.6218566894531\n",
      "RMSE:  0.0025551664\n",
      "Network coherency:  2.104174079420318e-08\n",
      "Last Layer Coherency Loss:  380.5447692871094\n",
      "RMSE:  0.002453224\n",
      "Network coherency:  1.8852338357078224e-08\n",
      "Last Layer Coherency Loss:  369.928955078125\n",
      "RMSE:  0.002525085\n",
      "Network coherency:  1.910760438761372e-08\n",
      "Last Layer Coherency Loss:  387.27484130859375\n",
      "RMSE:  0.0024191784\n",
      "Network coherency:  1.8802895240810358e-08\n",
      "Last Layer Coherency Loss:  400.7745056152344\n"
     ]
    }
   ],
   "source": [
    "n_trials = 5\n",
    "\n",
    "projection_metrics = np.zeros((4, n_trials))  \n",
    "projection_networks = [] \n",
    "\n",
    "for i in range(n_trials): \n",
    "    seed = random.randint(0, 100) \n",
    "    if trained_projection: \n",
    "        projection_network = load_model('projection_batched_{i}.pth'.format(i=i), True)\n",
    "    else: \n",
    "        projection_network, loss = train_net(500, batch_size=25, plot_loss=True, should_project=True, lr=0.0005)\n",
    "        save_model(projection_network, 'projection_batched_{i}'.format(i=i))\n",
    "        \n",
    "    rmse_projection, network_coherency_projection, network_loss_projection = get_metrics([projection_network], verbose=True)\n",
    "    validation_loss = calculate_RMSE(y_val.cpu().numpy(), projection_network(X_val.to(device)).detach().cpu().numpy())\n",
    "    projection_metrics[:, i] = [rmse_projection[0], network_coherency_projection[0], network_loss_projection[0], validation_loss]\n",
    "    projection_networks.append(rmse_projection) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.51106513e-03 1.86972637e-04 3.62971289e+02 1.91776259e-03]\n",
      "[5.33286060e-05 5.00039913e-05 9.19239817e+00 3.13659498e-05]\n",
      "[2.47891187e-03 1.93529274e-08 3.84628986e+02 1.89514745e-03]\n",
      "[5.20463454e-05 8.50851911e-10 1.00045004e+01 3.39326615e-05]\n"
     ]
    }
   ],
   "source": [
    "print(baseline_metrics.mean(axis=1)) \n",
    "print(baseline_metrics.std(axis=1)) \n",
    "print(projection_metrics.mean(axis=1))\n",
    "print(projection_metrics.std(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_experiments = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "alpha_scale_experiments = [0.05, .25, 1, 5, 20, 50]\n",
    "alpha_scale_increment_experiments = [0.1, 0.5, 1, 5, 10, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.0024238282\n",
      "Network coherency:  0.0013411894906312227\n",
      "Last Layer Coherency Loss:  348.1683654785156\n",
      "RMSE:  0.0025300297\n",
      "Network coherency:  0.0018778806552290916\n",
      "Last Layer Coherency Loss:  358.7933044433594\n",
      "RMSE:  0.0024762712\n",
      "Network coherency:  0.002354743890464306\n",
      "Last Layer Coherency Loss:  336.93212890625\n",
      "RMSE:  0.002531668\n",
      "Network coherency:  0.0019511979771777987\n",
      "Last Layer Coherency Loss:  340.25006103515625\n",
      "RMSE:  0.00538505\n",
      "Network coherency:  0.0019267895258963108\n",
      "Last Layer Coherency Loss:  351.9099426269531\n",
      "RMSE:  0.060927957\n",
      "Network coherency:  0.0010925795650109649\n",
      "Last Layer Coherency Loss:  336.7196044921875\n",
      "RMSE:  0.0024247284\n",
      "Network coherency:  0.0014417169149965048\n",
      "Last Layer Coherency Loss:  353.6699523925781\n",
      "RMSE:  0.0024587945\n",
      "Network coherency:  0.0019290943164378405\n",
      "Last Layer Coherency Loss:  352.8209228515625\n",
      "RMSE:  0.0024835882\n",
      "Network coherency:  0.0009874883107841015\n",
      "Last Layer Coherency Loss:  360.2652587890625\n",
      "RMSE:  0.002689871\n",
      "Network coherency:  0.000812465965282172\n",
      "Last Layer Coherency Loss:  343.59869384765625\n",
      "RMSE:  0.006781489\n",
      "Network coherency:  0.0017948633758351207\n",
      "Last Layer Coherency Loss:  352.45855712890625\n",
      "RMSE:  0.056354627\n",
      "Network coherency:  0.001002284698188305\n",
      "Last Layer Coherency Loss:  362.5483703613281\n",
      "RMSE:  0.0024149423\n",
      "Network coherency:  0.0005185475456528366\n",
      "Last Layer Coherency Loss:  364.7578125\n",
      "RMSE:  0.002462981\n",
      "Network coherency:  0.0009395530796609819\n",
      "Last Layer Coherency Loss:  355.364990234375\n",
      "RMSE:  0.0024809763\n",
      "Network coherency:  0.00020379130728542805\n",
      "Last Layer Coherency Loss:  358.66375732421875\n",
      "RMSE:  0.0026734993\n",
      "Network coherency:  0.0011085467413067818\n",
      "Last Layer Coherency Loss:  360.4066162109375\n",
      "RMSE:  0.0073034074\n",
      "Network coherency:  0.0017964451108127832\n",
      "Last Layer Coherency Loss:  373.03466796875\n",
      "RMSE:  0.0529971\n",
      "Network coherency:  0.000800809939391911\n",
      "Last Layer Coherency Loss:  370.0263977050781\n",
      "RMSE:  0.0024292276\n",
      "Network coherency:  0.0011011646129190922\n",
      "Last Layer Coherency Loss:  381.51129150390625\n",
      "RMSE:  0.0024395648\n",
      "Network coherency:  0.00020437120110727847\n",
      "Last Layer Coherency Loss:  365.5071105957031\n",
      "RMSE:  0.002486765\n",
      "Network coherency:  0.0015436027897521853\n",
      "Last Layer Coherency Loss:  346.0589599609375\n",
      "RMSE:  0.0026955\n",
      "Network coherency:  0.0016793465474620461\n",
      "Last Layer Coherency Loss:  352.182373046875\n",
      "RMSE:  0.0073324414\n",
      "Network coherency:  0.001844743499532342\n",
      "Last Layer Coherency Loss:  345.9891357421875\n",
      "RMSE:  0.05739933\n",
      "Network coherency:  0.0010126875713467598\n",
      "Last Layer Coherency Loss:  372.2070617675781\n",
      "RMSE:  0.0024104512\n",
      "Network coherency:  0.0013201069086790085\n",
      "Last Layer Coherency Loss:  341.2171630859375\n",
      "RMSE:  0.0024503334\n",
      "Network coherency:  0.0003071107785217464\n",
      "Last Layer Coherency Loss:  350.6949462890625\n",
      "RMSE:  0.002520559\n",
      "Network coherency:  0.0007476117461919785\n",
      "Last Layer Coherency Loss:  353.02569580078125\n",
      "RMSE:  0.0030145876\n",
      "Network coherency:  0.0011063024867326021\n",
      "Last Layer Coherency Loss:  346.4111328125\n",
      "RMSE:  0.010844648\n",
      "Network coherency:  0.0017510467441752553\n",
      "Last Layer Coherency Loss:  359.6544189453125\n",
      "RMSE:  0.056459665\n",
      "Network coherency:  0.0009100093157030642\n",
      "Last Layer Coherency Loss:  338.90936279296875\n"
     ]
    }
   ],
   "source": [
    "const_alpha = 0\n",
    "const_alph_scale = 0\n",
    "n_trials = 5\n",
    "alpha_inc_experiments_trained = True\n",
    "\n",
    "metrics_increment = np.zeros((4, n_trials, len(alpha_scale_increment_experiments)))\n",
    "networks_increment = [[] for i in range(n_trials)]\n",
    "\n",
    "for i in range(n_trials):  \n",
    "    scaling_idx = 0 \n",
    "    for scaling_increase_factor in alpha_scale_increment_experiments: \n",
    "        if alpha_inc_experiments_trained: \n",
    "            network = load_model('least_squares_scaling_increment_batched_{inc}_trial={i}.pth'.format(inc=scaling_increase_factor, i=i), False)\n",
    "        else: \n",
    "            network, loss = train_net(500, batch_size=25, least_squares=True, alpha=const_alpha, alpha_scaling=const_alph_scale, should_project=False,\n",
    "                                 scaling_increase_factor=scaling_increase_factor, \n",
    "                                 lr=0.001/(1 + np.log(1+scaling_increase_factor/4)), plot_loss=True)\n",
    "            save_model(network, 'least_squares_scaling_increment_batched_{inc}_trial={i}'.format(inc=scaling_increase_factor, i=i))\n",
    "            \n",
    "        rmse, coherency, network_loss = get_metrics([network], verbose=True)\n",
    "        validation_loss = calculate_RMSE(y_val.cpu().numpy(), network(X_val.to(device)).detach().cpu().numpy())\n",
    "        metrics_increment[:, i, scaling_idx] = [rmse[0], coherency[0], network_loss[0], validation_loss]\n",
    "        networks_increment[i].append(network) \n",
    "        \n",
    "        scaling_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "[0.00242064 0.00246834 0.00248963 0.00272103 0.00752941 0.05682774]\n",
      "[6.88334523e-06 3.18675630e-05 1.58407812e-05 1.58692073e-04\n",
      " 1.80212880e-03 2.53659634e-03]\n",
      "validation\n",
      "[0.00185579 0.00189197 0.00190495 0.00216175 0.00727826 0.05678795]\n",
      "[4.78599766e-06 2.24064092e-05 1.18862039e-05 1.27171743e-04\n",
      " 1.87407725e-03 2.53004873e-03]\n",
      "coherency\n",
      "[0.00114455 0.0010516  0.00116745 0.00133157 0.00182278 0.00096367]\n"
     ]
    }
   ],
   "source": [
    "print('test')\n",
    "print(metrics_increment.mean(axis=1)[0, :])\n",
    "print(metrics_increment.std(axis=1)[0, :])\n",
    "print('validation') \n",
    "print(metrics_increment.mean(axis=1)[3, :])\n",
    "print(metrics_increment.std(axis=1)[3, :])\n",
    "print('coherency') \n",
    "print(metrics_increment.mean(axis=1)[1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha Inc Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "--- alpha 0.0001 ---- lr  0.000975902419626165----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 101/500 [00:22<01:26,  4.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005312792097683996\n",
      "150.74275344848633\n",
      "365.61260986328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 110/500 [00:24<01:28,  4.42it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m     network \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaling_increment_batched_with_alpha_\u001b[39m\u001b[38;5;132;01m{inc}\u001b[39;00m\u001b[38;5;124m_trial=\u001b[39m\u001b[38;5;132;01m{i}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(inc\u001b[38;5;241m=\u001b[39mscaling_increase_factor, i\u001b[38;5;241m=\u001b[39mi), \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[0;32m---> 16\u001b[0m     network, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_net\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoherent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconst_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_scaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshould_project\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mscaling_increase_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaling_increase_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mscaling_increase_factor\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     save_model(network, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaling_increment_batched_with_alpha_\u001b[39m\u001b[38;5;132;01m{inc}\u001b[39;00m\u001b[38;5;124m_trial=\u001b[39m\u001b[38;5;132;01m{i}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(inc\u001b[38;5;241m=\u001b[39mscaling_increase_factor, i\u001b[38;5;241m=\u001b[39mi))\n\u001b[1;32m     21\u001b[0m rmse, coherency, network_loss \u001b[38;5;241m=\u001b[39m get_metrics([network], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[29], line 44\u001b[0m, in \u001b[0;36mtrain_net\u001b[0;34m(n_epochs, batch_size, coherent, alpha, should_project, alpha_scaling, least_squares, scaling_increase_factor, verbose, should_schedule, plot_loss, lr, max_grad_norm, update_alpha_epoch)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n_epochs)):  \u001b[38;5;66;03m# loop over the dataset multiple times\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_dataloader))\n\u001b[0;32m---> 44\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     45\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device) \n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# print(inputs[0, :20, 0])\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# print(labels[0, :20, 0]) \u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# print(X_train[:20, 0]) \u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# print(y_train[:20, 0]) \u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "const_alpha = .0001\n",
    "const_alph_scale = 0\n",
    "n_trials = 5\n",
    "alpha_inc_experiments_trained = False\n",
    "\n",
    "metrics_increment = np.zeros((4, n_trials, len(alpha_scale_increment_experiments)))\n",
    "networks_increment = [[] for i in range(n_trials)]\n",
    "\n",
    "for i in range(n_trials):  \n",
    "    scaling_idx = 0 \n",
    "    for scaling_increase_factor in alpha_scale_increment_experiments: \n",
    "        print(scaling_increase_factor)\n",
    "        if alpha_inc_experiments_trained: \n",
    "            network = load_model('scaling_increment_batched_with_alpha_{inc}_trial={i}.pth'.format(inc=scaling_increase_factor, i=i), False)\n",
    "        else: \n",
    "            network, loss = train_net(500, batch_size=25, coherent=True, alpha=const_alpha, alpha_scaling=0, should_project=False,\n",
    "                                 scaling_increase_factor=scaling_increase_factor, \n",
    "                                 lr=0.001/(1 + np.log(1+scaling_increase_factor/4)), plot_loss=True)\n",
    "            save_model(network, 'scaling_increment_batched_with_alpha_{inc}_trial={i}'.format(inc=scaling_increase_factor, i=i))\n",
    "            \n",
    "        rmse, coherency, network_loss = get_metrics([network], verbose=True)\n",
    "        validation_loss = calculate_RMSE(y_val.cpu().numpy(), network(X_val.to(device)).detach().cpu().numpy())\n",
    "        metrics_increment[:, i, scaling_idx] = [rmse[0], coherency[0], network_loss[0], validation_loss]\n",
    "        networks_increment[i].append(network) \n",
    "        \n",
    "        scaling_idx += 1\n",
    "\n",
    "metrics_increment.mean(axis=1), metrics_increment.std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "[0.00244228 0.00243055 0.00245881 0.00246045 0.00978913 0.05857045]\n",
      "[1.43627579e-05 1.77734658e-05 1.80429249e-05 2.73092936e-05\n",
      " 3.56392777e-03 2.08437220e-03]\n",
      "validation\n",
      "[0.00186398 0.00185939 0.00187608 0.00188426 0.00963269 0.0585307 ]\n",
      "[7.83302632e-06 1.12674273e-05 1.03151108e-05 1.34956171e-05\n",
      " 3.60564485e-03 2.08518335e-03]\n",
      "coherency\n",
      "[2.96579386e-05 2.64790906e-05 3.50264374e-05 3.38511181e-05\n",
      " 6.14801352e-04 6.38845906e-04]\n"
     ]
    }
   ],
   "source": [
    "print('test')\n",
    "print(metrics_increment.mean(axis=1)[0, :])\n",
    "print(metrics_increment.std(axis=1)[0, :])\n",
    "print('validation') \n",
    "print(metrics_increment.mean(axis=1)[3, :])\n",
    "print(metrics_increment.std(axis=1)[3, :])\n",
    "print('coherency') \n",
    "print(metrics_increment.mean(axis=1)[1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "RMSE:  0.0024289358\n",
      "Network coherency:  6.90147717250511e-05\n",
      "Last Layer Coherency Loss:  272.2029724121094\n",
      "0.5\n",
      "RMSE:  0.0024953096\n",
      "Network coherency:  0.00010145946725970134\n",
      "Last Layer Coherency Loss:  207.89576721191406\n",
      "1\n",
      "RMSE:  0.0024232902\n",
      "Network coherency:  5.120068817632273e-05\n",
      "Last Layer Coherency Loss:  189.04632568359375\n",
      "5\n",
      "RMSE:  0.0024539668\n",
      "Network coherency:  5.9286339819664136e-05\n",
      "Last Layer Coherency Loss:  158.20059204101562\n",
      "10\n",
      "RMSE:  0.0024485083\n",
      "Network coherency:  5.523706931853667e-05\n",
      "Last Layer Coherency Loss:  144.21359252929688\n",
      "50\n",
      "RMSE:  0.002839276\n",
      "Network coherency:  0.0001510406582383439\n",
      "Last Layer Coherency Loss:  108.645751953125\n",
      "0.1\n",
      "RMSE:  0.0025491898\n",
      "Network coherency:  0.00021103587641846389\n",
      "Last Layer Coherency Loss:  291.2292175292969\n",
      "0.5\n",
      "RMSE:  0.002425413\n",
      "Network coherency:  7.040808122837916e-05\n",
      "Last Layer Coherency Loss:  213.22467041015625\n",
      "1\n",
      "RMSE:  0.0024453355\n",
      "Network coherency:  6.2080733187031e-05\n",
      "Last Layer Coherency Loss:  182.7234649658203\n",
      "5\n",
      "RMSE:  0.0024523195\n",
      "Network coherency:  5.621148375212215e-05\n",
      "Last Layer Coherency Loss:  154.07766723632812\n",
      "10\n",
      "RMSE:  0.0025533095\n",
      "Network coherency:  7.821931649232283e-05\n",
      "Last Layer Coherency Loss:  144.0485382080078\n",
      "50\n",
      "RMSE:  0.002753355\n",
      "Network coherency:  0.0001921796938404441\n",
      "Last Layer Coherency Loss:  104.89093017578125\n",
      "0.1\n",
      "RMSE:  0.00245867\n",
      "Network coherency:  0.000105505452665966\n",
      "Last Layer Coherency Loss:  278.46142578125\n",
      "0.5\n",
      "RMSE:  0.0024633056\n",
      "Network coherency:  9.032624802784994e-05\n",
      "Last Layer Coherency Loss:  201.51055908203125\n",
      "1\n",
      "RMSE:  0.002481161\n",
      "Network coherency:  9.185331873595715e-05\n",
      "Last Layer Coherency Loss:  188.26898193359375\n",
      "5\n",
      "RMSE:  0.0024620518\n",
      "Network coherency:  6.423017475754023e-05\n",
      "Last Layer Coherency Loss:  156.80703735351562\n",
      "10\n",
      "RMSE:  0.00250636\n",
      "Network coherency:  7.953788008308038e-05\n",
      "Last Layer Coherency Loss:  146.20169067382812\n",
      "50\n",
      "RMSE:  0.0026611881\n",
      "Network coherency:  0.00010738406126620248\n",
      "Last Layer Coherency Loss:  116.06031036376953\n",
      "0.1\n",
      "RMSE:  0.002515634\n",
      "Network coherency:  0.0001567788131069392\n",
      "Last Layer Coherency Loss:  276.6230773925781\n",
      "0.5\n",
      "RMSE:  0.0024396952\n",
      "Network coherency:  7.155770435929298e-05\n",
      "Last Layer Coherency Loss:  197.00643920898438\n",
      "1\n",
      "RMSE:  0.0025160008\n",
      "Network coherency:  0.00010183786071138456\n",
      "Last Layer Coherency Loss:  182.48854064941406\n",
      "5\n",
      "RMSE:  0.0024429052\n",
      "Network coherency:  5.9487421822268516e-05\n",
      "Last Layer Coherency Loss:  163.62864685058594\n",
      "10\n",
      "RMSE:  0.0025243117\n",
      "Network coherency:  7.271912909345701e-05\n",
      "Last Layer Coherency Loss:  139.07205200195312\n",
      "50\n",
      "RMSE:  0.0029665225\n",
      "Network coherency:  0.00015238455671351403\n",
      "Last Layer Coherency Loss:  104.639892578125\n",
      "0.1\n",
      "RMSE:  0.002448617\n",
      "Network coherency:  9.677981870481744e-05\n",
      "Last Layer Coherency Loss:  290.2829284667969\n",
      "0.5\n",
      "RMSE:  0.0024885803\n",
      "Network coherency:  0.00011164695752086118\n",
      "Last Layer Coherency Loss:  215.294921875\n",
      "1\n",
      "RMSE:  0.0024966507\n",
      "Network coherency:  8.800464274827391e-05\n",
      "Last Layer Coherency Loss:  183.35525512695312\n",
      "5\n",
      "RMSE:  0.0024720775\n",
      "Network coherency:  6.874532118672505e-05\n",
      "Last Layer Coherency Loss:  162.04879760742188\n",
      "10\n",
      "RMSE:  0.0025843051\n",
      "Network coherency:  0.00010087163536809385\n",
      "Last Layer Coherency Loss:  150.84909057617188\n",
      "50\n",
      "RMSE:  0.002838794\n",
      "Network coherency:  0.0002018364320974797\n",
      "Last Layer Coherency Loss:  108.24943542480469\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2.48020934e-03, 2.46246075e-03, 2.47248760e-03, 2.45666415e-03,\n",
       "         2.52335891e-03, 2.81182714e-03],\n",
       "        [1.27822947e-04, 8.90796917e-05, 7.89954487e-05, 6.15921483e-05,\n",
       "         7.73170061e-05, 1.60965080e-04],\n",
       "        [2.81759924e+02, 2.06986472e+02, 1.85176514e+02, 1.58952548e+02,\n",
       "         1.44876993e+02, 1.08497264e+02],\n",
       "        [1.89105098e-03, 1.87963480e-03, 1.88900414e-03, 1.87992547e-03,\n",
       "         1.93062203e-03, 2.21520038e-03]]),\n",
       " array([[4.49537816e-05, 2.70288304e-05, 3.37937878e-05, 9.81894513e-06,\n",
       "         4.58344210e-05, 1.01510975e-04],\n",
       "        [5.03646337e-05, 1.62464969e-05, 1.91150333e-05, 4.40057367e-06,\n",
       "         1.46234817e-05, 3.37279250e-05],\n",
       "        [7.62773534e+00, 6.90447613e+00, 2.86699964e+00, 3.47522023e+00,\n",
       "         3.80117221e+00, 4.12718054e+00],\n",
       "        [2.80842392e-05, 1.62303932e-05, 2.51545771e-05, 6.36791559e-06,\n",
       "         3.26646888e-05, 1.01397398e-04]]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const_alpha = 0\n",
    "const_alph_scale = 0\n",
    "n_trials = 5\n",
    "alpha_inc_experiments_trained = True\n",
    "\n",
    "metrics_increment = np.zeros((4, n_trials, len(alpha_scale_increment_experiments)))\n",
    "networks_increment = [[] for i in range(n_trials)]\n",
    "\n",
    "for i in range(n_trials):  \n",
    "    scaling_idx = 0 \n",
    "    for scaling_increase_factor in alpha_scale_increment_experiments: \n",
    "        print(scaling_increase_factor)\n",
    "        if alpha_inc_experiments_trained: \n",
    "            network = load_model('scaling_increment_batched_{inc}_trial={i}.pth'.format(inc=scaling_increase_factor, i=i), False)\n",
    "        else: \n",
    "            network, loss = train_net(500, batch_size=25, coherent=True, alpha=0, alpha_scaling=0, should_project=False,\n",
    "                                 scaling_increase_factor=scaling_increase_factor, \n",
    "                                 lr=0.001/(1 + np.log(1+scaling_increase_factor/4)), plot_loss=True)\n",
    "            save_model(network, 'scaling_increment_batched_{inc}_trial={i}'.format(inc=scaling_increase_factor, i=i))\n",
    "            \n",
    "        rmse, coherency, network_loss = get_metrics([network], verbose=True)\n",
    "        validation_loss = calculate_RMSE(y_val.cpu().numpy(), network(X_val.to(device)).detach().cpu().numpy())\n",
    "        metrics_increment[:, i, scaling_idx] = [rmse[0], coherency[0], network_loss[0], validation_loss]\n",
    "        networks_increment[i].append(network) \n",
    "        \n",
    "        scaling_idx += 1\n",
    "\n",
    "metrics_increment.mean(axis=1), metrics_increment.std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "[0.00248021 0.00246246 0.00247249 0.00245666 0.00252336 0.00281183]\n",
      "[4.49537816e-05 2.70288304e-05 3.37937878e-05 9.81894513e-06\n",
      " 4.58344210e-05 1.01510975e-04]\n",
      "validation\n",
      "[0.00189105 0.00187963 0.001889   0.00187993 0.00193062 0.0022152 ]\n",
      "[2.80842392e-05 1.62303932e-05 2.51545771e-05 6.36791559e-06\n",
      " 3.26646888e-05 1.01397398e-04]\n",
      "coherency\n",
      "[1.27822947e-04 8.90796917e-05 7.89954487e-05 6.15921483e-05\n",
      " 7.73170061e-05 1.60965080e-04]\n"
     ]
    }
   ],
   "source": [
    "print('test')\n",
    "print(metrics_increment.mean(axis=1)[0, :])\n",
    "print(metrics_increment.std(axis=1)[0, :])\n",
    "print('validation') \n",
    "print(metrics_increment.mean(axis=1)[3, :])\n",
    "print(metrics_increment.std(axis=1)[3, :])\n",
    "print('coherency') \n",
    "print(metrics_increment.mean(axis=1)[1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'projection_inc_batched_0.1_trial=0.pth.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m alpha_scale_increment_experiments: \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m proj_inc_trained: \n\u001b[0;32m---> 13\u001b[0m         network \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprojection_inc_batched_\u001b[39;49m\u001b[38;5;132;43;01m{inc}\u001b[39;49;00m\u001b[38;5;124;43m_trial=\u001b[39;49m\u001b[38;5;132;43;01m{i}\u001b[39;49;00m\u001b[38;5;124;43m.pth.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43minc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaling_increase_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m     15\u001b[0m         network, loss \u001b[38;5;241m=\u001b[39m train_net(\u001b[38;5;241m2500\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, coherent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, alpha\u001b[38;5;241m=\u001b[39mconst_alpha, alpha_scaling\u001b[38;5;241m=\u001b[39mconst_alph_scale, should_project\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m                              scaling_increase_factor\u001b[38;5;241m=\u001b[39mscaling_increase_factor, \n\u001b[1;32m     17\u001b[0m                              lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mscaling_increase_factor\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m)), plot_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(path, should_project)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(path, should_project): \n\u001b[1;32m      2\u001b[0m     model \u001b[38;5;241m=\u001b[39m LstmNet(n_series, context_window, should_project\u001b[38;5;241m=\u001b[39mshould_project)\n\u001b[0;32m----> 3\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/lib/python3.8/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/lib/python3.8/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2022b/lib/python3.8/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'projection_inc_batched_0.1_trial=0.pth.pth'"
     ]
    }
   ],
   "source": [
    "const_alpha = 0\n",
    "const_alph_scale = 0\n",
    "n_trials = 5\n",
    "proj_inc_trained = True\n",
    "\n",
    "metrics_project_increment = np.zeros((4, n_trials, len(alpha_scale_increment_experiments)))\n",
    "networks_project_increment = [[] for i in range(n_trials)]\n",
    "\n",
    "for i in range(n_trials):  \n",
    "    scaling_idx = 0 \n",
    "    for alpha in alpha_scale_increment_experiments: \n",
    "        if proj_inc_trained: \n",
    "            network = load_model('projection_inc_batched_{inc}_trial={i}.pth.pth'.format(inc=scaling_increase_factor, i=i), True)\n",
    "        else: \n",
    "            network, loss = train_net(2500, batch_size=100, coherent=True, alpha=const_alpha, alpha_scaling=const_alph_scale, should_project=True,\n",
    "                                 scaling_increase_factor=scaling_increase_factor, \n",
    "                                 lr=0.005/(1 + np.log(1+scaling_increase_factor/4)), plot_loss=True)\n",
    "            save_model(network, 'projection_inc_batched_{inc}_trial={i}.pth'.format(inc=scaling_increase_factor, i=i))\n",
    "            \n",
    "        rmse, coherency, network_loss = get_metrics([network], verbose=True)\n",
    "        validation_loss = calculate_RMSE(y_val.numpy(), network(X_val.to(device)).detach().cpu().numpy())\n",
    "        metrics_project_increment[:, i, scaling_idx] = [rmse[0], coherency[0], network_loss[0], validation_loss]\n",
    "        networks_project_increment[i].append(network)\n",
    "        \n",
    "        scaling_idx += 1\n",
    "\n",
    "metrics_project_increment.mean(axis=1), metrics_project_increment.std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_experiments = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.0024118687\n",
      "Network coherency:  1.9645873180706985e-05\n",
      "Last Layer Coherency Loss:  90.32659912109375\n",
      "RMSE:  0.002424522\n",
      "Network coherency:  8.87392889126204e-06\n",
      "Last Layer Coherency Loss:  34.90675354003906\n",
      "RMSE:  0.06458228\n",
      "Network coherency:  0.002961063990369439\n",
      "Last Layer Coherency Loss:  39.402034759521484\n",
      "RMSE:  0.08790663\n",
      "Network coherency:  0.0013986353296786547\n",
      "Last Layer Coherency Loss:  32.92516326904297\n",
      "RMSE:  0.088913\n",
      "Network coherency:  0.0023632559459656477\n",
      "Last Layer Coherency Loss:  42.30485534667969\n",
      "RMSE:  0.0889752\n",
      "Network coherency:  0.002204327378422022\n",
      "Last Layer Coherency Loss:  38.24991989135742\n",
      "RMSE:  0.0024124552\n",
      "Network coherency:  2.6186375180259347e-05\n",
      "Last Layer Coherency Loss:  91.17963409423828\n",
      "RMSE:  0.0024174543\n",
      "Network coherency:  1.232714748766739e-05\n",
      "Last Layer Coherency Loss:  48.386146545410156\n",
      "RMSE:  0.06475026\n",
      "Network coherency:  0.001886898186057806\n",
      "Last Layer Coherency Loss:  23.255077362060547\n",
      "RMSE:  0.09139318\n",
      "Network coherency:  0.0016038106987252831\n",
      "Last Layer Coherency Loss:  25.87582015991211\n",
      "RMSE:  0.0914732\n",
      "Network coherency:  0.0027899467386305332\n",
      "Last Layer Coherency Loss:  40.81262969970703\n",
      "RMSE:  0.08943238\n",
      "Network coherency:  0.00213699322193861\n",
      "Last Layer Coherency Loss:  36.801780700683594\n",
      "RMSE:  0.0024448258\n",
      "Network coherency:  3.7444136978592724e-05\n",
      "Last Layer Coherency Loss:  95.69229125976562\n",
      "RMSE:  0.00240993\n",
      "Network coherency:  5.841452093591215e-06\n",
      "Last Layer Coherency Loss:  19.76422691345215\n",
      "RMSE:  0.06545278\n",
      "Network coherency:  0.0018427877221256495\n",
      "Last Layer Coherency Loss:  25.965744018554688\n",
      "RMSE:  0.08866358\n",
      "Network coherency:  0.001215403201058507\n",
      "Last Layer Coherency Loss:  28.716670989990234\n",
      "RMSE:  0.09021707\n",
      "Network coherency:  0.0026841151993721724\n",
      "Last Layer Coherency Loss:  41.54024124145508\n",
      "RMSE:  0.089263596\n",
      "Network coherency:  0.002223034156486392\n",
      "Last Layer Coherency Loss:  37.95719528198242\n",
      "RMSE:  0.002468276\n",
      "Network coherency:  4.332617754698731e-05\n",
      "Last Layer Coherency Loss:  90.63836669921875\n",
      "RMSE:  0.0024171495\n",
      "Network coherency:  8.07317246653838e-06\n",
      "Last Layer Coherency Loss:  35.319419860839844\n",
      "RMSE:  0.06433675\n",
      "Network coherency:  0.002223493065685034\n",
      "Last Layer Coherency Loss:  31.0184268951416\n",
      "RMSE:  0.088967495\n",
      "Network coherency:  0.0017161160940304399\n",
      "Last Layer Coherency Loss:  34.96699142456055\n",
      "RMSE:  0.09048831\n",
      "Network coherency:  0.0026604316662997007\n",
      "Last Layer Coherency Loss:  41.81251907348633\n",
      "RMSE:  0.08943269\n",
      "Network coherency:  0.002278710948303342\n",
      "Last Layer Coherency Loss:  41.20185470581055\n",
      "RMSE:  0.0024324257\n",
      "Network coherency:  3.104314237134531e-05\n",
      "Last Layer Coherency Loss:  92.1171875\n",
      "RMSE:  0.0024208906\n",
      "Network coherency:  1.2284004696994089e-05\n",
      "Last Layer Coherency Loss:  46.59323501586914\n",
      "RMSE:  0.063187934\n",
      "Network coherency:  0.003927925601601601\n",
      "Last Layer Coherency Loss:  33.90523147583008\n",
      "RMSE:  0.088157445\n",
      "Network coherency:  0.0013197114458307624\n",
      "Last Layer Coherency Loss:  28.90500259399414\n",
      "RMSE:  0.088807054\n",
      "Network coherency:  0.002946068299934268\n",
      "Last Layer Coherency Loss:  45.89533996582031\n",
      "RMSE:  0.09017048\n",
      "Network coherency:  0.002457000780850649\n",
      "Last Layer Coherency Loss:  38.98798751831055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[2.43397029e-03, 2.41798931e-03, 6.44620001e-02, 8.90176654e-02,\n",
       "         8.99797261e-02, 8.94548684e-02],\n",
       "        [3.15291411e-05, 9.47994113e-06, 2.56843371e-03, 1.45073535e-03,\n",
       "         2.68876357e-03, 2.26001330e-03],\n",
       "        [9.19908157e+01, 3.69939564e+01, 3.07093029e+01, 3.02779297e+01,\n",
       "         4.24731171e+01, 3.86397476e+01],\n",
       "        [1.86164740e-03, 1.84915699e-03, 6.44288659e-02, 8.89822260e-02,\n",
       "         8.99403200e-02, 8.94160882e-02]]),\n",
       " array([[2.12061228e-05, 4.83811560e-06, 7.37310291e-04, 1.24461613e-03,\n",
       "         1.00583410e-03, 3.94909239e-04],\n",
       "        [8.29639252e-06, 2.51213885e-06, 7.88975736e-04, 1.83914055e-04,\n",
       "         1.91427937e-04, 1.08402941e-04],\n",
       "        [1.94800626e+00, 1.02562392e+01, 5.72607952e+00, 3.24641869e+00,\n",
       "         1.77785190e+00, 1.46161756e+00],\n",
       "        [1.33124272e-05, 1.63163244e-06, 7.38225864e-04, 1.24419225e-03,\n",
       "         1.00836374e-03, 3.97088181e-04]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const_alpha = 0\n",
    "const_alph_scale = 0\n",
    "n_trials = 5\n",
    "alpha_inc_experiments_trained = True\n",
    "\n",
    "metrics_increment = np.zeros((4, n_trials, len(alpha_experiments)))\n",
    "networks_increment = [[] for i in range(n_trials)]\n",
    "\n",
    "for i in range(n_trials):  \n",
    "    scaling_idx = 0 \n",
    "    for alpha in alpha_experiments: \n",
    "        if alpha_inc_experiments_trained: \n",
    "            network = load_model('alpha_{alpha}_trial={i}.pth'.format(alpha=alpha, i=i), False)\n",
    "        else: \n",
    "            network, loss = train_net(500, batch_size=25, coherent=True, alpha=alpha, alpha_scaling=const_alph_scale, should_project=False,\n",
    "                                 scaling_increase_factor=5, \n",
    "                                 lr=0.005/(1 + np.log(1+scaling_increase_factor/4)), plot_loss=True)\n",
    "            save_model(network, 'alpha_{alpha}_trial={i}'.format(alpha=alpha, i=i))\n",
    "            \n",
    "        rmse, coherency, network_loss = get_metrics([network], verbose=True)\n",
    "        validation_loss = calculate_RMSE(y_val.cpu().numpy(), network(X_val.to(device)).detach().cpu().numpy())\n",
    "        metrics_increment[:, i, scaling_idx] = [rmse[0], coherency[0], network_loss[0], validation_loss]\n",
    "        networks_increment[i].append(network) \n",
    "        \n",
    "        scaling_idx += 1\n",
    "\n",
    "metrics_increment.mean(axis=1), metrics_increment.std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "[0.00243397 0.00241799 0.064462   0.08901767 0.08997973 0.08945487]\n",
      "[2.12061228e-05 4.83811560e-06 7.37310291e-04 1.24461613e-03\n",
      " 1.00583410e-03 3.94909239e-04]\n",
      "validation\n",
      "[0.00186165 0.00184916 0.06442887 0.08898223 0.08994032 0.08941609]\n",
      "[1.33124272e-05 1.63163244e-06 7.38225864e-04 1.24419225e-03\n",
      " 1.00836374e-03 3.97088181e-04]\n",
      "coherency\n",
      "[3.15291411e-05 9.47994113e-06 2.56843371e-03 1.45073535e-03\n",
      " 2.68876357e-03 2.26001330e-03]\n"
     ]
    }
   ],
   "source": [
    "print('test')\n",
    "print(metrics_increment.mean(axis=1)[0, :])\n",
    "print(metrics_increment.std(axis=1)[0, :])\n",
    "print('validation') \n",
    "print(metrics_increment.mean(axis=1)[3, :])\n",
    "print(metrics_increment.std(axis=1)[3, :])\n",
    "print('coherency') \n",
    "print(metrics_increment.mean(axis=1)[1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_alpha = 0\n",
    "const_alph_scale = 0\n",
    "\n",
    "networks_scale_increment = [train_net(10000, coherent=True, alpha=const_alpha, alpha_scaling=const_alph_scale, \n",
    "                            scaling_increase_factor=scaling_increase_factor, lr=0.005/(1 + np.log(1 + scaling_increase_factor/4)), \n",
    "                                      plot_loss=True) for scaling_increase_factor in [10]]\n",
    "\n",
    "get_metrics(networks_scale_increment, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rmses_if = np.mean(metrics_increment[0], axis=0)\n",
    "std_rmses_if = np.std(metrics_increment[0], axis=0)\n",
    "\n",
    "print(mean_rmses_if)\n",
    "print(std_rmses_if) \n",
    "\n",
    "mean_network_coherencies_if = np.mean(metrics_increment[1], axis=0)\n",
    "std_network_coherencies_if = np.std(metrics_increment[1], axis=0)\n",
    "\n",
    "mean_ll_losses_if = np.mean(metrics_increment[2], axis=0)\n",
    "std_ll_losses_if = 2*np.std(metrics_increment[2], axis=0)\n",
    "\n",
    "print(mean_network_coherencies_if)\n",
    "print(std_network_coherencies_if) \n",
    "\n",
    "# Calculate mean and std for baseline and projection metrics\n",
    "mean_metrics_baseline = np.mean(baseline_metrics, axis=1)\n",
    "std_metrics_baseline = 2*np.std(baseline_metrics, axis=1)\n",
    "\n",
    "mean_metrics_projection = np.mean(projection_metrics, axis=1)\n",
    "std_metrics_projection = 2*np.std(projection_metrics, axis=1)\n",
    "\n",
    "print(mean_metrics_baseline)\n",
    "print(std_metrics_baseline)\n",
    "print(mean_metrics_projection)\n",
    "print(std_metrics_projection) \n",
    "\n",
    "# Plotting\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(6, 10))\n",
    "\n",
    "# RMSE plot with error bars\n",
    "ax1.set_title('RMSE vs alpha scaling increment')\n",
    "ax1.errorbar(alpha_scale_increment_experiments, mean_rmses_if, yerr=std_rmses_if, fmt='o', label='Incremental')\n",
    "ax1.axhline(mean_metrics_baseline[0], color='red', label='Baseline', linestyle='--')\n",
    "ax1.axhline(mean_metrics_projection[0], color='blue', label='Projection', linestyle='--')\n",
    "ax1.fill_between([min(alpha_scale_increment_experiments), max(alpha_scale_increment_experiments)], mean_metrics_baseline[0]-std_metrics_baseline[0], mean_metrics_baseline[0]+std_metrics_baseline[0], color='red', alpha=0.2)\n",
    "ax1.fill_between([min(alpha_scale_increment_experiments), max(alpha_scale_increment_experiments)], mean_metrics_projection[0]-std_metrics_projection[0], mean_metrics_projection[0]+std_metrics_projection[0], color='blue', alpha=0.2)\n",
    "ax1.set_ylabel('RMSE')\n",
    "ax1.set_xlabel('Alpha')\n",
    "ax1.legend()\n",
    "\n",
    "# Network coherency plot with error bars\n",
    "ax2.set_title('Coherency on data vs alpha, incremental')\n",
    "ax2.errorbar(alpha_scale_increment_experiments, mean_network_coherencies_if, yerr=std_network_coherencies_if, fmt='o')\n",
    "ax2.set_ylabel('Coherency on data')\n",
    "ax2.set_xlabel('Alpha')\n",
    "\n",
    "# Last layer coherency plot with error bars\n",
    "ax3.errorbar(alpha_scale_increment_experiments, mean_ll_losses_if, yerr=std_ll_losses_if, fmt='o')\n",
    "ax3.set_ylabel('Last layer coherency')\n",
    "ax3.set_xlabel('Alpha')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_alpha = 0\n",
    "const_alph_scale = 0\n",
    "\n",
    "networks_project = [train_net(6000, coherent=True, alpha=0, alpha_scaling=0, should_project=True, plot_loss=True,\n",
    "                            scaling_increase_factor=scaling_increase_factor, lr=0.005/(1 + np.log(1+scaling_increase_factor/4))) for scaling_increase_factor in alpha_scale_increment_experiments]\n",
    "\n",
    "get_metrics(networks_project, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(6, 10))\n",
    "ax1.set_title('metrics vs alpha scaling increment with projection')\n",
    "ax1.scatter(alpha_scale_experiments, rmses_if)\n",
    "ax1.axhline(rmse_baseline, color='red', label='baseline')\n",
    "ax1.axhline(rmse_projection, color='green', label='projection')\n",
    "ax1.set_ylabel('rmse')\n",
    "ax1.set_xlabel('alpha')\n",
    "ax1.legend()\n",
    "#ax2.set_title('coherency on data vs alpha, no scaling and no scaling increment')\n",
    "ax2.scatter(alpha_scale_experiments, network_coherencies_if)\n",
    "ax2.set_ylabel('coherency on data')\n",
    "ax2.set_xlabel('alpha')\n",
    "#ax3.set_title('last layer coherency vs alpha, no scaling and no scaling increment')\n",
    "ax3.scatter(alpha_scale_experiments, ll_losses_if)\n",
    "ax3.set_ylabel('last layer coherency')\n",
    "ax3.set_xlabel('alpha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network, losses = train_net(1000, coherent=False) \n",
    "#network_coherent, losses_coherent = train_net(1000, coherent=True, alpha=0)\n",
    "#network_coherent_high_scaling, losses_coherent_high_scaling = train_net(1000, coherent=True, alpha=0.25, alpha_scaling=1)\n",
    "network_projection, losses_projection = train_net(5000, should_project=True)\n",
    "#network_coherent_and_projecting, losses_projection_high_scaling = train_net(1000, coherent=True, alpha=0.5, alpha_scaling=1, should_project=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(losses_projection[100:])), losses_projection[100:], label='projection')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "#plt.plot(range(len(losses_coherent[101:])), losses_coherent[101:], label='coherent')\n",
    "#plt.plot(range(len(losses_coherent_high_scaling[101:])), losses_coherent_high_scaling[101:], label='high scaling')\n",
    "#plt.legend() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_projection = calculate_mean_RMSE(network_projection)\n",
    "rmse_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_projection = 0.0032 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_baseline, losses_baseline = train_net(2000, coherent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_baseline = calculate_mean_RMSE(network_baseline)\n",
    "rmse_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize=(18, 10))\n",
    "fig.suptitle('Labour Dataset')\n",
    "\n",
    "# First Set of Data\n",
    "axs[0, 0].set_title('Alpha Experiments (No Scaling and No Scaling Increment)')\n",
    "axs[0, 0].scatter(alpha_experiments, rmses)\n",
    "axs[0, 0].set_ylabel('rmse')\n",
    "axs[0, 0].set_xlabel('alpha')\n",
    "axs[0, 0].axhline(rmse_baseline, color='red', label='baseline')\n",
    "axs[0, 0].axhline(rmse_projection, color='green', label='projection')\n",
    "axs[0, 0].legend()\n",
    "\n",
    "axs[1, 0].scatter(alpha_experiments, network_coherencies)\n",
    "axs[1, 0].set_ylabel('coherency on data')\n",
    "axs[1, 0].set_xlabel('alpha')\n",
    "axs[1, 0].axhline(network_coherency_baseline, color='red', label='baseline')\n",
    "axs[1, 0].legend()\n",
    "\n",
    "axs[2, 0].scatter(alpha_experiments, ll_losses)\n",
    "axs[2, 0].set_ylabel('last layer coherency')\n",
    "axs[2, 0].set_xlabel('alpha')\n",
    "\n",
    "# Second Set of Data\n",
    "axs[0, 1].set_title('Alpha Scaling Experiments (Constant alpha = 0)')\n",
    "axs[0, 1].scatter(alpha_scale_experiments, rmses_s)\n",
    "axs[0, 1].axhline(rmse_baseline, color='red', label='baseline')\n",
    "axs[0, 1].axhline(rmse_projection, color='green', label='projection')\n",
    "axs[0, 1].set_ylabel('rmse')\n",
    "axs[0, 1].set_xlabel('alpha scaling')\n",
    "axs[0, 1].legend()\n",
    "\n",
    "axs[1, 1].scatter(alpha_scale_experiments, network_coherencies_s)\n",
    "axs[1, 1].set_ylabel('coherency on data')\n",
    "axs[1, 1].set_xlabel('alpha scaling')\n",
    "axs[1, 1].axhline(network_coherency_baseline, color='red', label='baseline')\n",
    "axs[1, 1].legend()\n",
    "\n",
    "axs[2, 1].scatter(alpha_scale_experiments, ll_losses_s)\n",
    "axs[2, 1].set_ylabel('last layer coherency')\n",
    "axs[2, 1].set_xlabel('alpha scaling')\n",
    "\n",
    "# Third Set of Data\n",
    "axs[0, 2].set_title('Scaling Increment Experiments (Constant alpha = 0, scaling = 0)')\n",
    "axs[0, 2].scatter(alpha_scale_increment_experiments, rmses_if)\n",
    "axs[0, 2].axhline(rmse_baseline, color='red', label='baseline')\n",
    "axs[0, 2].axhline(rmse_projection, color='green', label='projection')\n",
    "axs[0, 2].set_ylabel('rmse')\n",
    "axs[0, 2].set_xlabel('alpha increment')\n",
    "axs[0, 2].legend()\n",
    "\n",
    "axs[1, 2].scatter(alpha_scale_increment_experiments, network_coherencies_if)\n",
    "axs[1, 2].set_ylabel('coherency on data')\n",
    "axs[1, 2].set_xlabel('alpha increment')\n",
    "axs[1, 2].axhline(network_coherency_baseline, color='red', label='baseline')\n",
    "axs[1, 2].legend()\n",
    "\n",
    "axs[2, 2].scatter(alpha_scale_increment_experiments, ll_losses_if)\n",
    "axs[2, 2].set_ylabel('last layer coherency')\n",
    "axs[2, 2].set_xlabel('alpha increment')\n",
    "\n",
    "axs[0,0].set_xscale('log')\n",
    "axs[0,1].set_xscale('log')\n",
    "axs[0,2].set_xscale('log')\n",
    "axs[1,0].set_xscale('log')\n",
    "axs[1,1].set_xscale('log')\n",
    "axs[1,2].set_xscale('log')\n",
    "axs[2,0].set_xscale('log')\n",
    "axs[2,1].set_xscale('log')\n",
    "axs[2,2].set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('parameter_experiments_traffic.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_alpha = 0\n",
    "const_alph_scale = 0\n",
    "\n",
    "networks_increment_scaled = [train_net(4000, coherent=True, alpha=0, alpha_scaling=0, should_project=True,\n",
    "                            scaling_increase_factor=scaling_increase_factor + 0.1, lr=0.005/(1 + np.log(1+scaling_increase_factor))) for scaling_increase_factor in alpha_scale_experiments]\n",
    "\n",
    "get_metrics(networks_increment_scaled, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement a function that chooses alpha based on the best training data and hopefully show that the selected alpha also performs well on test data\n",
    "\n",
    "def test_best_alpha(networks):\n",
    "\n",
    "    train_rmses = [] \n",
    "    test_rmses = [] \n",
    "\n",
    "    for network, _ in networks: \n",
    "\n",
    "        network.lstm.eval() \n",
    "        train_rmse = calculate_RMSE(y_val.numpy(), network(X_val.to(device)).detach().cpu().numpy())\n",
    "        test_rmse = calculate_RMSE(y_test.numpy(), network(X_test.to(device)).detach().cpu().numpy())\n",
    "\n",
    "        train_rmses.append(train_rmse)\n",
    "        test_rmses.append(test_rmse)\n",
    "\n",
    "    return train_rmses, test_rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmses, test_rmses = test_best_alpha(networks_scale_increment)\n",
    "#train_rmses_scaled, test_rmses_scaled = test_best_alpha(networks_increment_scaled)\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(8, 6))  # Adjust the figure size as needed\n",
    "plt.scatter(train_rmses, test_rmses)\n",
    "\n",
    "plt.axhline(rmse_projection, color='green', label='projection')\n",
    "plt.axhline(rmse_baseline, color='red', label='baseline')\n",
    "\n",
    "# Annotate each point with its label\n",
    "for i, label in enumerate(alpha_scale_increment_experiments):\n",
    "    plt.annotate(label, (train_rmses[i], test_rmses[i]), textcoords=\"offset points\", xytext=(0, 5), ha='center')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Test RMSE vs. Validation RMSE. Labels are Alpha Scaling Increment Values')\n",
    "plt.xlabel('Validation RMSE')\n",
    "plt.ylabel('Test RMSE')\n",
    "\n",
    "# # Set custom axis limits\n",
    "# plt.xlim(0.0010, 0.0013)\n",
    "# plt.ylim(0.004, 0.0055)\n",
    "\n",
    "# Show the plot\n",
    "plt.legend() \n",
    "plt.savefig('val_traffic.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_best_alpha(networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses for learning\n",
    "#plt.plot(range(len(losses[100:])), losses[100:], label='base')\n",
    "plt.plot(range(len(losses_projection[100:1000])), losses_projection_high_scaling[100:1000], label='projection')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(range(len(losses_coherent[101:])), losses_coherent[101:], label='coherent')\n",
    "plt.plot(range(len(losses_coherent_high_scaling[101:])), losses_coherent_high_scaling[101:], label='high scaling')\n",
    "plt.legend() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the real coherency of each network we trained\n",
    "print(\"baseline network coherency\", coherency_metric(network(X_test)).item())\n",
    "print(\"coherent network coherency\", coherency_metric(network_coherent(X_test)).item())\n",
    "print(\"coherent network coherency (high scaling)\", coherency_metric(network_coherent_high_scaling(X_test)).item())\n",
    "#print(\"projection network coherency\", coherency_metric(network_projection(X_test)).item())\n",
    "#print(\"both coherency\", coherency_metric(network_coherent_and_projecting(X_test)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 20\n",
    "\n",
    "def calculate_mean_RMSE(network, n_iters=n_iters): \n",
    "    errs = np.zeros((n_iters, )) \n",
    "    for i in range(n_iters): \n",
    "        errs[i] = calculate_RMSE(y_test.numpy(), network(X_test).detach().numpy())\n",
    "    return np.mean(errs)\n",
    "\n",
    "print(\"RMSE Baseline:\", calculate_mean_RMSE(network))\n",
    "print(\"RMSE Coherent:\", calculate_mean_RMSE(network_coherent))\n",
    "print(\"RMSE Coherent (high scaling):\", calculate_mean_RMSE(network_coherent_high_scaling))\n",
    "print(\"RMSE Projection:\", calculate_mean_RMSE(network_projection))\n",
    "print(\"RMSE Both:\", calculate_mean_RMSE(network_coherent_and_projecting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"WMAPE Baseline:\", calculate_wmape(y_test.numpy(), network(X_test).detach().numpy()))\n",
    "print(\"WMAPE Coherent:\", calculate_wmape(y_test.numpy(), network_coherent(X_test).detach().numpy()))\n",
    "print(\"WMAPE Coherent (high scaling):\", calculate_wmape(y_test.numpy(), network_coherent_high_scaling(X_test).detach().numpy()))\n",
    "print(\"WMAPE Projection:\", calculate_wmape(y_test.numpy(), network_projection(X_test).detach().numpy()))\n",
    "print(\"WMAPE Both:\", calculate_wmape(y_test.numpy(), network_coherent_and_projecting(X_test).detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Last Layer Coherency Loss Baseline:\", coherency_loss(network).item())\n",
    "print(\"Last Layer Coherency Loss Coherent:\", coherency_loss(network_coherent).item())\n",
    "print(\"Last Layer Coherency Loss Coherent (high scaling):\", coherency_loss(network_coherent_high_scaling).item())\n",
    "print(\"Last Layer Coherency Loss Projection:\", coherency_loss(network_projection).item())\n",
    "print(\"Last Layer Coherency Loss Both:\", coherency_loss(network_coherent_and_projecting).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training 'least squares loss' networks \n",
    "network_ls, loss_ls = train_net(500, least_squares=True, alpha=0.5, alpha_scaling=1)\n",
    "network_ls_high_scaling, loss_ls_high_scaling = train_net(500, least_squares=True, alpha=0.5, alpha_scaling=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the real coherency of each network we trained\n",
    "print(\"MSE_loss network coherency\", coherency_metric(network_ls(X_test)).item())\n",
    "print(\"MSE_loss high scaling network coherency\", coherency_metric(network_ls_high_scaling(X_test)).item())\n",
    "print(\"MSE_loss and project coherency\", coherency_metric(network_ls_and_project(X_test)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the real coherency of each network we trained\n",
    "print(\"baseline network RMSE\", calculate_RMSE(y_test.numpy(), network_ls(X_test).detach().numpy()))\n",
    "print(\"coherent network RMSE\", calculate_RMSE(y_test.numpy(), network_ls_high_scaling(X_test).detach().numpy()))\n",
    "print(\"coherent network coherency (high scaling) RMSE\", calculate_RMSE(y_test.numpy(), network_ls_and_project(X_test).detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Last Layer Coherency Loss Coherent:\", coherency_loss(network_ls).item())\n",
    "print(\"Last Layer Coherency Loss Coherent (high scaling):\", coherency_loss(network_ls_high_scaling).item())\n",
    "print(\"Last Layer Coherency Loss Projection:\", coherency_loss(network_projection).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(0, [network, network_coherent, network_projection], ['baseline', 'coherent loss', 'projection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_agg_levels = np.unique([torch.sum(aggregation_mat[ts,:]).item() for ts in range(len(aggregation_mat))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_agg_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code plots the error \n",
    "\n",
    "aggregation_level = all_agg_levels[0]\n",
    "aggregation_amounts = []\n",
    "errs_baseline = [] \n",
    "errs_coherent = []\n",
    "for ts in range(len(aggregation_mat)):\n",
    "    agg_level = torch.sum(aggregation_mat[ts,:]).item()\n",
    "    if agg_level == aggregation_level: \n",
    "        err_baseline = torch.norm(network(X_test)[:,ts] - y_test[:,ts]).item() / len(X_test)\n",
    "        err_coherent = torch.norm(network_coherent(X_test)[:,ts] - y_test[:,ts]).item() / len(X_test)\n",
    "\n",
    "        aggregation_amounts.append(agg_level)\n",
    "        errs_baseline.append(err_baseline)\n",
    "        errs_coherent.append(err_coherent)\n",
    "\n",
    "plt.hist([errs_baseline, errs_coherent], label = ['projection', 'coherent'])\n",
    "\n",
    "print(\"Baseline:\", np.mean(errs_baseline))\n",
    "print(\"Coherent:\", np.mean(errs_coherent))\n",
    "\n",
    "plt.xlabel('|y_pred - y|/N')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
